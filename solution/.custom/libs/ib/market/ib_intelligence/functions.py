import os
from typing import Dict, List, Any, Text, Optional, Type
import json
import uuid
import hashlib

from google.protobuf.json_format import MessageToJson

from instabase.model_service.sdk import ModelService
from instabase.provenance.tracking import Value, ImageProvenanceTracker, create_image_position
from instabase.data_api_utils._sheet.types.model_utils import generic_model_result_to_ner_result
from instabase.ocr.client.libs.ibocr import ParsedIBOCRBuilder
from instabase.protos.model_service import model_service_pb2
from instabase.refiner_utils.types.refiner_constants import CONFIG


def get_model_service(**kwargs: Any) -> ModelService:
  context = kwargs['_FN_CONTEXT_KEY']
  # TODO: Once auth key approach is available, use API instead
  username = context.get_username()

  clients, err = kwargs['_FN_CONTEXT_KEY'].get_by_col_name('CLIENTS')
  fs_params = None
  if not err:
    if clients.ibfile:
      fs_params = clients.ibfile._fs_params
  try:
    # Added try catch statement for backwards compatibility.
    # If fs_params isn't supported by model service sdk,
    # then we are on an older release. We fallback to older model service sdk.
    ms_api = ModelService(username=username, fs_params=fs_params)
  except Exception:
    ms_api = ModelService(username=username)
  return ms_api


BUILTIN_KWARGS = set(['_FN_CONTEXT_KEY', 'version'])


def kwargs_to_ms_params(kwargs: Any) -> Dict[Any, Any]:
  return {
      key: Value.unwrap_value(val)
      for key, val in kwargs.items() if key not in BUILTIN_KWARGS
  }


def resolve_input(input_text: str, input_record: Any, input_path: str) -> Dict:
  if input_text is not None:
    return {'input_string': input_text}
  if input_record is not None:  # THIS SHOULD BE A PARSED_IBOCR
    ibocr = input_record.serialize_to_string()
    raw_ibocr = model_service_pb2.RawData(type='ibocr', data=ibocr)
    return {'input_raw_data': json.loads(MessageToJson(raw_ibocr))}
  if input_path is not None:
    return {'input_path': input_path}

  return {}


import logging  # this is just for the constants logging.INFO and logging.DEBUG


def log(message: Text, **kwargs: Any) -> None:
  logger, err = kwargs['_FN_CONTEXT_KEY'].get_by_col_name('LOGGER')
  if err:
    return

  # make sure you are calling loggER.log and not loggING.info
  logger.log(logging.INFO, message)


## CONVERTERS --------------------------------------------------


def convert_NERResult_to_values(input_col: Value[str],
                                model_result: Dict) -> Value[List[Value[str]]]:
  print(model_result)
  return Value([
      input_col[e['start_index']:e['end_index']]
      for e in model_result.value()["ner_result"]["entities"]  # type: ignore
  ])


CONVERTERS = {('NERResult', 'Values'): convert_NERResult_to_values}

################################################################


def convert_data(data: Value[Any], from_type: str, to_type: str, origin: Any,
                 **kwargs: Any) -> Any:
  """Converts responses from the Intelligence Platform into various types.
  """
  conversion_type = (Value.unwrap_value(from_type),
                     Value.unwrap_value(to_type))
  if conversion_type not in CONVERTERS:
    raise Exception('Cannot convert from {} to {}'.format(
        conversion_type[0], conversion_type[1]))
  return CONVERTERS[conversion_type](origin, data)  # type: ignore


def provenance_track_region(value: Any, x1: int, y1: int, x2: int, y2: int,
                            page: int) -> Value[Any]:
  """
  pixel bounding box layout is here:
      x1            x2
   y1 +-------------+
      |             |
   y2 +-------------+

   and "page" is the page of the *original document*
   (note: that may be different from the page within the record)
  """

  new_value = Value(value)
  result_location = create_image_position(x1, y1, x2, y2, page)
  new_value.set_image_tracker(ImageProvenanceTracker(result_location))
  return new_value


class IntelligencePlatform:
  """The IntelligencePlatform class provides an context-linked interface for
  calling model from UDFs and Refiner programs.
  """

  def __init__(self,
               udf_kwargs: Dict,
               model_locations: List[str] = None,
               use_model_service_lite: bool = False) -> None:
    self.api = get_model_service(**udf_kwargs)
    self.udf_kwargs = udf_kwargs
    if model_locations:
      logging.warning('model_locations is deprecated! Please provide a model '
                      'name and version in run_model instead')
    # Only to maintain compatibility with refiner scripts generated by older
    # versions of ibllm which expect model-service-lite to be present
    # TODO (yashb): Remove this once all apps and scripts have been migrated to
    # use LLMSession
    self.use_model_service_lite = use_model_service_lite

  def run_model(self,
                model_name: str,
                input_text: str = None,
                input_record: Any = None,
                input_path: str = None,
                model_version: str = None,
                is_project_model: bool = False,
                model_fs_path: str = None,
                custom_request: Optional[Dict] = None,
                run_external_model: bool = False,
                **kwargs: Any) -> Any:
    """Runs a model on a single input.

    Note: Only one of the following should be provided:
          - input_text
          - input_record
          - input_path

    Args:
      model_name (str): The name of the model to run (required)
      input_text (str): Text to run the model on (i.e. INPUT_COL)
      input_record (ParsedIBOCRBuilder): An ParsedIBOCRBuilder object with 1
        record to run on the model on.
        (i.e. INPUT_IBOCR_RECORD, but in an ParsedIBOCRBuilder object)
      input_path (str): The path to a file to run on
      model_version (str): The version of the model to run. Required.
      is_project_model (bool): The type of the model to run. (False for the Marketplace model, True for the project model)
      model_fs_path (str): The file system path for the project model. Required only when is_project_model = True
      custom_request (dict): A dictionary of custom parameters to directly pass to the model
      run_external_model (bool): Whether to run the model as an external model (default: False)

    Returns:
      The result of running the given model on the given input
    """

    if not model_version:
      raise Exception('Did not provide a model_version, which is required.')
    if 'force_reload' in kwargs:
      logging.warning(
          'force_reload is provided, but is deprecated. Will be ignored from the input parameters.'
      )
    if is_project_model and not model_fs_path:
      raise Exception(
          'Need to provide the model_fs_path for the project model.')
    if custom_request is None:
      custom_request = {}

    parameters = kwargs_to_ms_params(kwargs)
    custom_request.update(parameters)
    content_dict = resolve_input(input_text, input_record, input_path)
    fn_ctx = kwargs.get('_FN_CONTEXT_KEY')
    job_id, err = fn_ctx.get_by_col_name('JOB_ID')
    if err:
      logging.warning(err)
    task_id, err = fn_ctx.get_by_col_name('TASK_ID')
    if err:
      logging.warning(err)
    root_output_folder, err = fn_ctx.get_by_col_name('ROOT_OUTPUT_FOLDER')
    if err:
      logging.warning(err)

    runtime_config, err = fn_ctx.get_by_col_name(CONFIG)
    if err:
      logging.warning(f'Error getting runtime config: {err}')
    if not runtime_config:
      logging.warning('Runtime config is empty, using default values')
      runtime_config = {}
    app_release_version = runtime_config.get('app_release_version', '')
    custom_request['app_release_version'] = app_release_version

    req = {
        'context': {
            'username': self.api.username,
            'orgname': fn_ctx.get_orgname(),
        },
        'modelName': model_name,
        'modelVersion': model_version,
        'external_model': self.use_model_service_lite,
        'resultCacheFolder': root_output_folder,
        'isProjectModel': is_project_model,
        'modelFsPath': model_fs_path,
        'jobId': job_id,
        'taskId': task_id,
        'modelPayload': {
            'customRequest': custom_request
        },
        'external_model': self.use_model_service_lite or run_external_model
    }

    # Log ibllm request but without ibdoc details because the input_raw_data
    # field may be too large
    logging.info(
        f"Calling ibllm with the following request (excluding ibdoc details): {req}"
    )
    req.update(content_dict)
    try:
      result = self.api.run_json(req)['model_result']
    except Exception as e:
      err_msg = 'Error running model {}-{}: {}'.format(model_name,
                                                       model_version, e)
      log(err_msg, **kwargs)
      raise Exception(err_msg)

    # Refiner UDF scripts generated on platform < 22.11 cannot handle the
    # predictions in 'generic_model_result', which was introduced in 22.11,
    # so we make sure that the 'ner_result' is populated with the same
    # prediction data.
    #
    # For more context, see https://instabase.atlassian.net/browse/INSIGHTS-5009
    if 'generic_model_result' in result and 'ner_result' not in result:
      result['ner_result'] = generic_model_result_to_ner_result(
          result['generic_model_result'])

    return result

  def get_person_names(self, input_text: str, **kwargs: Any) -> Any:
    result = self.run_model('ib_person_name', input_text=input_text, **kwargs)
    return result

  def get_cached_index(self) -> str:
    """
    This function serves as the only place refiner cached_indices are constructed for 24.01+. This function is called in LLMSession.__init__(), and LLMSession is instantiated in ib_llm_tools (to add/delete docs) and the post_flow_cleanup task (to delete indices)
    """

    fn_ctx = self.udf_kwargs.get('_FN_CONTEXT_KEY')
    job_id, err = fn_ctx.get_by_col_name('JOB_ID')
    if err:
      logging.warning(err)

    if job_id:
      cleaned_job_id = job_id.replace("-", "_")
      return f"GPT_run_flow_{cleaned_job_id}"

    # if job_id is missing from fn_ctx (aka dev refiner run), return dev cached index
    file_path = fn_ctx.get_by_col_name('INPUT_FILEPATH')[0]
    parent_folder_path = os.path.split(file_path)[0]
    hex_string = hashlib.md5(parent_folder_path.encode("UTF-8")).hexdigest()
    uuid_str = str(uuid.UUID(hex=hex_string)).replace("-", "_")
    return f"GPT_run_flow_dev_{uuid_str}"

  class LLMSession:
    # hard coded ibllm version tight to the platform
    IBLLM_NAME: str = os.environ.get('AIHUB_MODEL_NAME', "ibllm")
    IBLLM_VERSION: str = os.environ.get('AIHUB_MODEL_VERSION', "2.1.0")

    def __init__(self,
                 ip_sdk: Type['IntelligencePlatform'],
                 index_name: Optional[str] = None,
                 reuse_index: Optional[bool] = True) -> None:
      """Create a LLM session with given records.
      A LLM session is a context manager that:
        - create index name if not provided
        - deleting index after use
        - pick the right version of ibllm which is tight to the platform

      Args:
        ip_sdk: IntelligencePlatform instance
        index_name: name of existing index, if None, the name will be generated **This argument shouldn't be used for new initializations, maintained only for backwards compatibility with ib_llm_tools <0.2.0**
        reuse_index: if False, then delete index during __exit__
      """
      self.ip_sdk = ip_sdk
      self.index_name = index_name if index_name else self.ip_sdk.get_cached_index()  # type: ignore
      self.reuse_index = reuse_index

    def run_llm(self,
                custom_request: Dict,
                input_text: str = None,
                input_record: Any = None,
                input_path: str = None) -> Any:
      """Run ibllm task with the index and record within this session
      """
      request: Dict[str, Any] = {
          # hard coded ibllm version for the platform
          'model_name': IntelligencePlatform.LLMSession.IBLLM_NAME,
          'model_version': IntelligencePlatform.LLMSession.IBLLM_VERSION,
          'run_external_model': True,
          # LLM input, only one of the following should be provided:
          'input_text': input_text,
          'input_record': input_record,
          'input_path': input_path,
          # Custom args for the LLM task
          'custom_request': {
              "cached_index": self.index_name,
              **custom_request
          },
      }
      custom_result = self.ip_sdk.run_model(**request, **self.ip_sdk.udf_kwargs)["custom_result"]  # type: ignore
      return custom_result

    def add_record(self,
                   record: ParsedIBOCRBuilder,
                   check_exists: bool = False,
                   record_number: str = None,
                   custom_request: Dict = {}) -> str:
      """Helper function to add records to the index
      """
      request = {
          'model_name': IntelligencePlatform.LLMSession.IBLLM_NAME,
          'model_version': IntelligencePlatform.LLMSession.IBLLM_VERSION,
          "run_external_model": True,
          "input_record": record,
          'custom_request': {
              "task": "add_to_index",
              "cached_index": self.index_name,
              "check_exists": check_exists,
              "record_number": record_number,
              **custom_request
          },
      }
      custom_result = self.ip_sdk.run_model(**request, **self.ip_sdk.udf_kwargs)["custom_result"]  # type: ignore

      return custom_result['error']

    def delete_record(self, input_path: str) -> str:
      """ Delete records from the index
      Inside a index, each record is identified by its path
      """
      request = {
          'model_name': IntelligencePlatform.LLMSession.IBLLM_NAME,
          'model_version': IntelligencePlatform.LLMSession.IBLLM_VERSION,
          "run_external_model": True,
          "input_path": input_path,
          'custom_request': {
              "task": "delete_from_index",
              "cached_index": self.index_name
          },
      }
      custom_result = self.ip_sdk.run_model(**request, **self.ip_sdk.udf_kwargs)["custom_result"]  # type: ignore

      return custom_result['error']

    def delete_index(self) -> str:
      """ Delete entire index
      """
      request = {
          'model_name': IntelligencePlatform.LLMSession.IBLLM_NAME,
          'model_version': IntelligencePlatform.LLMSession.IBLLM_VERSION,
          "run_external_model": True,
          'custom_request': {
              "task": "delete_class_index",
              "cached_index": self.index_name
          },
      }
      custom_result = self.ip_sdk.run_model(**request, **self.ip_sdk.udf_kwargs)["custom_result"]  # type: ignore

      return custom_result['error']

    def __enter__(self):  # type: ignore
      """ In each session, use the cached index to run the queries
      """
      return self

    def __exit__(self, type, value, traceback) -> None:  # type: ignore
      """ delete the index after use
      """
      if not self.reuse_index:
        request = {
            'model_name': IntelligencePlatform.LLMSession.IBLLM_NAME,
            'model_version': IntelligencePlatform.LLMSession.IBLLM_VERSION,
            "run_external_model": True,
            'custom_request': {
                "task": "delete_class_index",
                "cached_index": self.index_name
            },
        }
        self.ip_sdk.run_model(**request, **self.ip_sdk.udf_kwargs)  # type: ignore

  def open_llm_session(self, index_name: Optional[str] = None):  # type: ignore
    """Create a LLM session.
    A LLM session is a context manager that:
      - create index name if not provided
      - deleting index after use
      - pick the right version of ibllm which is tight to the platform

    Example:
      with ip_sdk.open_llm_session() as session:
        session.run_llm_model(
          custom_request={
            "model_name": "gpt-3.5-turbo",
            "fields": [
              {
                "name": "summary",
                "prompt": "What is the summary of this doc?",
                "type": "advanced"
              }
            ],
			      "task": "extraction"
		      },
          input_text="<input text from document>")
    """
    return IntelligencePlatform.LLMSession(self, index_name=index_name)  # type: ignore


def register_intelligence_platform(name_to_fn: Dict,
                                   model_locations: List[str] = None) -> None:
  """Registers the Instabase Intelligence Platform, sourcing models from the
  given model_locations, using the auth_token provided.
  """

  if model_locations:
    logging.warning('model_locations is provided, but is deprecated. Please '
                    'only reference a model by name and version.')

  def run_model(model_name: Value[str],
                input_text: Value[str] = Value(None),
                input_record: Value[Any] = Value(None),
                input_path: Value[str] = Value(None),
                model_version: Value[str] = Value(None),
                is_project_model: Value[bool] = Value(False),
                model_fs_path: Value[str] = Value(None),
                **kwargs: Any) -> Any:
    """Runs a model on a single input.

    Note: Only one of the following should be provided:
          - input_text
          - input_record
          - input_path

    Args:
      model_name (str): The name of the model to run (required)
      input_text (str): Text to run the model on (i.e. INPUT_COL)
      input_record (IBOCRRecord): A record to run on (i.e. INPUT_IBOCR_RECORD)
      input_path (str): The path to a file to run on
      model_version(str): The version of the model to run
      is_project_model (bool): The type of the model to run. (False for the Marketplace model, True for the project model)
      model_fs_path (str): The file system path for the project model. Required only when is_project_model = True

    Returns:
      The result of running the given model on the given input
    """

    kwargs_unwrapped = Value.unwrap_value(kwargs)
    contextual_int = IntelligencePlatform(kwargs_unwrapped, model_locations)
    return contextual_int.run_model(
        model_name.value(),
        input_text=input_text.value(),
        input_record=input_record.value(),
        input_path=input_path.value(),
        model_version=model_version.value(),
        is_project_model=is_project_model.value(),
        model_fs_path=model_fs_path.value(),
        **kwargs_unwrapped)

  def run_model_unwrapped(model_name: str,
                          input_text: str = None,
                          input_record: Any = None,
                          input_path: str = None,
                          model_version: str = None,
                          is_project_model: bool = False,
                          model_fs_path: str = None,
                          **kwargs: Any) -> Any:
    return Value.unwrap_value(
        run_model(
            Value(model_name), Value(input_text), Value(input_record),
            Value(input_path), Value(model_version), Value(is_project_model),
            Value(model_fs_path), **kwargs))

  def get_person_names(input_text: Value[str], **kwargs: Any) -> Value[Any]:
    kwargs_unwrapped = Value.unwrap_value(kwargs)
    contextual_int = IntelligencePlatform(kwargs_unwrapped, model_locations)
    result = contextual_int.get_person_names(input_text.value(),
                                             **kwargs_unwrapped)
    return Value(result)

  name_to_fn.update({
      'run_model': {
          'fn_v': run_model,
          'fn': run_model_unwrapped,
          'ex': '',
          'desc': ''
      },
      'get_person_names': {
          'fn_v': get_person_names,
          'fn': lambda *args, **kwargs: "Please use provenance tracking",
          'ex': '',
          'desc': ''
      },
      'convert_data': {
          'fn_v': convert_data,
          'fn': convert_data,
          'ex': '',
          'desc': ''
      }
  })


def register(name_to_fn: Dict) -> None:
  model_paths = [
      'vontell/model-service/fs/Instabase Drive/ib-models/',
  ]
  register_intelligence_platform(name_to_fn, model_paths)
